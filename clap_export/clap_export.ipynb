{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Globals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "tauri_onnx_models_directory = \"../SonicSearch/src-tauri/onnx_models/\"\n",
    "file_timestamp = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "model_name = \"laion/clap-htsat-unfused\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utilities\n",
    "import time\n",
    "\n",
    "def make_filename_or_dirname(filename, extension=None):\n",
    "    extension = \"\" if extension is None else \".\" + extension.strip('.')\n",
    "    filename = filename.strip('.').lstrip('/')\n",
    "    return f'{tauri_onnx_models_directory}{filename}{extension}'\n",
    "\n",
    "# Inspect inputs and outputs\n",
    "def get_shapes_in_nests(node, count=0):\n",
    "    try:\n",
    "        return str(node.shape)\n",
    "    except:\n",
    "        count += 1\n",
    "        try:\n",
    "            return ('\\n' + '\\t'*count).join([f'{key}: {get_shapes_in_nests(value)}' for key, value in node.items()])\n",
    "        except:\n",
    "            if isinstance(node, list):\n",
    "                return ('\\n' + '\\t'*count).join([get_shapes_in_nests(n) for n in node])\n",
    "            else:\n",
    "                return str(node)\n",
    "        \n",
    "class QuickTimer():\n",
    "    \"\"\"hahaha\"\"\"\n",
    "    _start = 0\n",
    "    \n",
    "    def start():\n",
    "        QuickTimer._start = time.time()\n",
    "    \n",
    "    def stop():\n",
    "        return time.time() - QuickTimer._start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedders: Audio + Text Model with Projection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, ClapTextModelWithProjection\n",
    "\n",
    "\n",
    "text_model = ClapTextModelWithProjection.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "tokenized_inputs = tokenizer([\"the longest input one would reasonably use. Truly just one loooooooong input :).\"], padding=True, return_tensors=\"pt\")\n",
    "\n",
    "text_model_outputs = text_model(**tokenized_inputs)\n",
    "text_embeds = text_model_outputs.text_embeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text inputs and outputs\n",
    "\n",
    "print(\"Inputs: \", get_shapes_in_nests(tokenized_inputs))\n",
    "print(\"Outputs: \", get_shapes_in_nests(text_model_outputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Onnx Export - Text Model with projection\n",
    "\n",
    "from torch import onnx\n",
    "\n",
    "print(\"Exporting tokenizer config...\")\n",
    "tokenizer.save_pretrained(make_filename_or_dirname(\"tokenizer\"))\n",
    "\n",
    "print(\"Exporting model to ONNX...\")\n",
    "QuickTimer.start()\n",
    "onnx.export(\n",
    "    text_model,\n",
    "    (tokenized_inputs[\"input_ids\"], tokenized_inputs[\"attention_mask\"]),\n",
    "    make_filename_or_dirname(f\"{model_name.split('/')[-1]}_text_with_projection\", \"onnx\"),\n",
    "    export_params=True,\n",
    "    input_names=[\"input_ids\", \"attention_mask\"],\n",
    "    output_names=[\"text_embeds\", \"last_hidden_state\"],\n",
    "    dynamic_axes={\n",
    "        \"input_ids\": {0: \"batch_size\", 1: \"sequence_length\"},\n",
    "        \"attention_mask\": {0: \"batch_size\", 1: \"sequence_length\"},\n",
    "        \"text_embeds\": {0: \"batch_size\"},\n",
    "        \"last_hidden_state\": {0: \"batch_size\"}\n",
    "    }\n",
    ")\n",
    "print(\"Exporting model to ONNX took: \", QuickTimer.stop())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import ClapAudioModelWithProjection, ClapProcessor\n",
    "import numpy as np\n",
    "\n",
    "audio_model = ClapAudioModelWithProjection.from_pretrained(model_name)\n",
    "processor = ClapProcessor.from_pretrained(model_name)\n",
    "\n",
    "dataset = load_dataset(\"ashraq/esc50\")\n",
    "audio_sample = [datum[\"array\"] for datum in dataset[\"train\"][\"audio\"][0:31]]\n",
    "longer_sample = np.concatenate(np.array([datum[\"array\"] for datum in dataset[\"train\"][\"audio\"][32:34]]))\n",
    "audio_sample.append(longer_sample)\n",
    "\n",
    "audio_inputs = processor(audios=audio_sample, return_tensors=\"pt\", sampling_rate=48000)\n",
    "audio_outputs = audio_model(**audio_inputs)\n",
    "audio_embeds = audio_outputs.audio_embeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Audio inputs and outputs\n",
    "\n",
    "print(\"Inputs: \", get_shapes_in_nests(audio_inputs))\n",
    "print(\"Outputs: \", get_shapes_in_nests(audio_outputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Onnx Export - Audio Model with projection\n",
    "\n",
    "from torch import onnx\n",
    "\n",
    "print(\"Exporting feature extractor config...\")\n",
    "processor.feature_extractor.save_pretrained(make_filename_or_dirname(\"feature_extractor\"))\n",
    "\n",
    "print(\"Exporting model to ONNX...\")\n",
    "QuickTimer.start()\n",
    "onnx.export(\n",
    "    audio_model,\n",
    "    (audio_inputs[\"input_features\"], audio_inputs[\"is_longer\"]),\n",
    "    make_filename_or_dirname(f\"{model_name.split('/')[-1]}_audio_with_projection\", \"onnx\"),\n",
    "    export_params=True,\n",
    "    input_names=[\"input_features\", \"is_longer\"],\n",
    "    output_names=[\"audio_embeds\", \"last_hidden_state\"],\n",
    "    dynamic_axes={'input_features': {0: 'batch_size'},\n",
    "                  'is_longer': {0: 'batch_size'},\n",
    "                  'audio_embeds': {0: 'batch_size'},\n",
    "                  'last_hidden_state': {0: 'batch_size'},\n",
    "                  }\n",
    ")\n",
    "print(\"Exporting model to ONNX took: \", QuickTimer.stop())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SCRATCH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Audio Processor Understanding\n",
    "My hypothesis: when audio is <10 seconds, and we aren't doing fusion, the preprocessing is simply repeat-padding the values and transforming them to a Mel Spectrogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(get_shapes_in_nests(audio_sample[-5:]))\n",
    "print(get_shapes_in_nests(audio_inputs[\"input_features\"][-5:]))\n",
    "print(get_shapes_in_nests(audio_inputs[\"is_longer\"][-5:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# An attempt to manually reverse-engineer the preprocessing\n",
    "\n",
    "import librosa\n",
    "from IPython.display import Audio, display\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "sample_num = 1\n",
    "\n",
    "librosa.display.plt.figure(figsize=(20, 20))\n",
    "librosa.display.plt.subplot(2, 2, 1)\n",
    "librosa.display.plt.title('Preprocessed audio_sample')\n",
    "print(\"Preprocessed audio_sample shape: \", audio_sample[sample_num].shape)\n",
    "display(Audio(data=audio_sample[sample_num], rate=44100))\n",
    "librosa.display.waveshow(audio_sample[sample_num], sr=44100, color='blue')\n",
    "\n",
    "librosa.display.plt.subplot(2, 2, 3)\n",
    "librosa.display.plt.title('Processed input_features')\n",
    "print(\"Processed input_features shape: \", audio_inputs[\"input_features\"][sample_num].shape)\n",
    "specshow_tensor = torch.transpose(audio_inputs[\"input_features\"][sample_num,0], 0, 1).numpy()\n",
    "librosa.display.specshow(specshow_tensor, sr=44100, x_axis='time', y_axis='mel', hop_length=480, cmap='coolwarm', fmax=14000, fmin=50, n_fft=1024, win_length=1024)\n",
    "\n",
    "librosa.display.plt.subplot(2, 2, 2)\n",
    "librosa.display.plt.title('Slice')\n",
    "librosa.display.plt.plot(specshow_tensor[:, 100], color=\"blue\")\n",
    "\n",
    "\n",
    "def pad_mel_dbifier(manual_processing_inputs):\n",
    "    manual_processed_input_features = []\n",
    "    for manual_processing_input in manual_processing_inputs:\n",
    "        max_length = 48000 * 10\n",
    "        assert manual_processing_input.shape[0] <= max_length, \"Input is too long\"\n",
    "        if manual_processing_input.shape[0] < max_length:\n",
    "                    n_repeat = int(max_length / len(manual_processing_input))\n",
    "                    stacked_manual_processing_input = np.stack(np.tile(manual_processing_input, n_repeat))\n",
    "                    padded_manual_processing_input = np.pad(stacked_manual_processing_input, (0, max_length - manual_processing_input.shape[0]))\n",
    "        else:\n",
    "            padded_manual_processing_input = manual_processing_input\n",
    "        sample_melled = librosa.feature.melspectrogram(y=padded_manual_processing_input, sr=48000, n_fft=1024, hop_length=480, win_length=1024, window='hann', norm='slaney', n_mels=64, power=2.0) # \"mel\"\n",
    "        sample_melled_and_dbed = librosa.power_to_db(sample_melled) # TODO: resulting values look lower; inspect\n",
    "        manual_processed_input_feature = np.expand_dims(sample_melled_and_dbed.transpose(), 0)\n",
    "        features_np_array = manual_processed_input_feature[:, :1001, :] # HACkHACkHACkHACk put me in jail\n",
    "        manual_processed_input_features.append(features_np_array)\n",
    "        # TODO: trunkation is weird; inspect\n",
    "\n",
    "    return {\"input_features\": torch.from_numpy(np.stack(manual_processed_input_features, axis=0)).to(torch.float32), \"is_longer\": torch.Tensor([False] * len(manual_processing_inputs)).unsqueeze(1).to(torch.bool)}\n",
    "\n",
    "manual_processed_input_features = pad_mel_dbifier(audio_sample[sample_num-1:sample_num+1]) # slicing for speed :/\n",
    "\n",
    "librosa.display.plt.subplot(2, 2, 4)\n",
    "librosa.display.plt.title('Manually Processed audio_sample')\n",
    "print(\"Manual processed input_features shape: \", manual_processed_input_features[\"input_features\"][sample_num].shape)\n",
    "manually_processed_specshow_tensor = torch.transpose(manual_processed_input_features[\"input_features\"][sample_num,0], 0, 1).numpy()\n",
    "librosa.display.specshow(manually_processed_specshow_tensor, sr=44100, x_axis='time', y_axis='mel', hop_length=480, cmap='coolwarm', fmax=14000, fmin=50, n_fft=1024, win_length=1024)\n",
    "\n",
    "librosa.display.plt.subplot(2, 2, 2)\n",
    "librosa.display.plt.plot(manually_processed_specshow_tensor[:, 100], color=\"red\")\n",
    "\n",
    "\n",
    "librosa.display.plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import librosa\n",
    "import librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers.audio_utils import mel_filter_bank\n",
    "\n",
    "# Configuration parameters\n",
    "n_fft = 1024\n",
    "n_mels = 64\n",
    "sr = 48000\n",
    "fmin = 50\n",
    "fmax = 14000\n",
    "\n",
    "# Your mel_filter_bank function needs to be defined as you have it\n",
    "\n",
    "# Generate the Mel filter banks using both librosa and your custom function\n",
    "librosa_filters = librosa.filters.mel(sr=sr, n_fft=n_fft, n_mels=n_mels, fmin=fmin, fmax=fmax)\n",
    "your_mel_filters = mel_filter_bank(\n",
    "    num_frequency_bins=513,\n",
    "    num_mel_filters=64,\n",
    "    max_frequency=14000,\n",
    "    min_frequency=50,\n",
    "    sampling_rate=48000,\n",
    "    mel_scale='slaney',\n",
    "    norm='slaney',\n",
    ").transpose() # This should be the output of your custom function\n",
    "\n",
    "# Plotting both filter banks for comparison\n",
    "plt.figure(figsize=(20, 10))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "librosa.display.specshow(librosa_filters, sr=sr, hop_length=512, x_axis='log')\n",
    "plt.title('Librosa Mel Filter Bank')\n",
    "plt.colorbar(format='%+2.0f dB')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "librosa.display.specshow(your_mel_filters, sr=sr, hop_length=512, x_axis='log')\n",
    "plt.title('Your Mel Filter Bank')\n",
    "plt.colorbar(format='%+2.0f dB')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "librosa.display.specshow(librosa_filters - your_mel_filters, sr=sr, hop_length=512, x_axis='log')\n",
    "plt.title('Difference between filter banks')\n",
    "plt.colorbar(format='%+2.0f dB')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...oh well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(get_shapes_in_nests(audio_inputs))\n",
    "print(audio_inputs[\"input_features\"].dtype)\n",
    "print(audio_inputs[\"is_longer\"].dtype)\n",
    "print(get_shapes_in_nests(manual_processed_input_features))\n",
    "print(manual_processed_input_features[\"input_features\"].dtype)\n",
    "print(manual_processed_input_features[\"is_longer\"].dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actual Model Assessment\n",
    "I went to all this trouble... does this model even work well?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def assess_model(text_input, text_preprocessor, text_model, audio_input, audio_preprocessor, audio_model):\n",
    "    text_embeds = text_model(text_preprocessor(text_input))\n",
    "    audio_embeds = audio_model(audio_preprocessor(audio_input))\n",
    "\n",
    "    for i in range(len(text_input)):\n",
    "        plt.subplot(len(text_input), 1,i+1)\n",
    "        plt.title(f\"{i}: {text_input[i]}\")\n",
    "        plt.imshow(text_embeds[i].reshape(8,-1).detach().numpy())\n",
    "    plt.show()\n",
    "    \n",
    "    for i in range(len(audio_input[0:5])):\n",
    "        plt.subplot(len(audio_input[0:5]), 1,i+1)\n",
    "        plt.title(f\"Audio {i}\")\n",
    "        plt.imshow(audio_embeds[i].reshape(8,-1).detach().numpy())\n",
    "    plt.show()\n",
    "\n",
    "    # Cosine Similarities\n",
    "\n",
    "    norm_text_embeds = F.normalize(text_embeds, p=2, dim=1)\n",
    "    norm_audio_embeds = F.normalize(audio_embeds, p=2, dim=1)\n",
    "\n",
    "    cosine_similarities=F.cosine_similarity(text_embeds.unsqueeze(1), audio_embeds.unsqueeze(0), dim=2)\n",
    "    plt.title(\"Cosine Similarities\")\n",
    "    plt.imshow(cosine_similarities.detach().numpy())\n",
    "    plt.xlabel(\"Audio\")\n",
    "    plt.ylabel(\"Text\")\n",
    "    plt.show()\n",
    "\n",
    "    # Top-3 and Bottom-3 Cosine Similarities for each text input\n",
    "    for i in range(len(text_input)):\n",
    "        print(text_input[i])\n",
    "        print(\"Top 3\")\n",
    "        top_3_indices = cosine_similarities[i].argsort(descending=True)[0:3]\n",
    "        print(top_3_indices.tolist())\n",
    "        for j in top_3_indices:\n",
    "            display(Audio(data=audio_sample[j], rate=44100))\n",
    "        print(\"Bottom 3\")\n",
    "        bottom_3_indices = cosine_similarities[i].argsort(descending=False)[0:3]\n",
    "        print(bottom_3_indices.tolist())\n",
    "        for j in bottom_3_indices:\n",
    "            display(Audio(data=audio_sample[j], rate=44100))\n",
    "        print()\n",
    "        \n",
    "    return text_embeds, audio_embeds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assess ONNX Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reverse-engineered processor + Onnx model\n",
    "\n",
    "import onnxruntime\n",
    "\n",
    "text_ort_session = onnxruntime.InferenceSession(make_filename_or_dirname(f\"{model_name.split('/')[-1]}_text_with_projection\", \"onnx\"))\n",
    "audio_ort_session = onnxruntime.InferenceSession(make_filename_or_dirname(f\"{model_name.split('/')[-1]}_audio_with_projection\", \"onnx\"))\n",
    "\n",
    "print([input.name for input in text_ort_session.get_inputs()])\n",
    "print([output.name for output in text_ort_session.get_outputs()])\n",
    "print([input.name for input in audio_ort_session.get_inputs()])\n",
    "print([output.name for output in audio_ort_session.get_outputs()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def np_dict_to_ortvalue(dict):\n",
    "   return {key: onnxruntime.OrtValue.ortvalue_from_numpy(value.numpy() if isinstance(value, torch.Tensor) else value) for key, value in dict.items()} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare preprocessors\n",
    "text_input = [\"the sound of a gunshot\", \"the sound of a crowd\", \"the sound of a dog\", \"the sound of a bird\", \"the sound of applause\", \"the sound of a car\"]\n",
    "audio_input = [datum[\"array\"] for datum in dataset[\"train\"][\"audio\"][0:31]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from_pretrained tokenizer and processor\n",
    "QuickTimer.start()\n",
    "print(\"from_pretrained tokenizer and audio processor\")\n",
    "hf_text_embeds, hf_audio_embeds = assess_model(\n",
    "    text_input=text_input,\n",
    "    text_preprocessor=lambda input: tokenizer(input, padding=True, return_tensors=\"pt\"),\n",
    "    text_model=lambda processed_input: text_model(**processed_input).text_embeds,\n",
    "    audio_input=audio_sample,\n",
    "    audio_preprocessor=lambda input: processor(audios=input, return_tensors=\"pt\", sampling_rate=48000),\n",
    "    audio_model=lambda processed_input: audio_model(**processed_input).audio_embeds\n",
    ")\n",
    "print(\"from_pretrained tokenizer and audio processor took: \", QuickTimer.stop())\n",
    "\n",
    "# Reverse-engineered processor\n",
    "QuickTimer.start()\n",
    "print(\"Reverse-engineered audio processor\")\n",
    "re_text_embeds, re_audio_embeds = assess_model(\n",
    "    text_input=text_input,\n",
    "    text_preprocessor=lambda input: tokenizer(input, padding=True, return_tensors=\"pt\"),\n",
    "    text_model=lambda processed_input: text_model(**processed_input).text_embeds,\n",
    "    audio_input=audio_sample,\n",
    "    audio_preprocessor=lambda input: pad_mel_dbifier(input),\n",
    "    audio_model=lambda processed_input: audio_model(**processed_input).audio_embeds\n",
    ")\n",
    "print(\"Reverse-engineered audio processor took: \", QuickTimer.stop())\n",
    "\n",
    "# Onnx processor\n",
    "QuickTimer.start()\n",
    "print(\"Onnx audio processor\")\n",
    "ort_text_embeds, ort_audio_embeds = assess_model(\n",
    "    text_input=text_input,\n",
    "    text_preprocessor=lambda input: np_dict_to_ortvalue(tokenizer(input, padding=True, return_tensors=\"np\", pad_to_multiple_of=20)),\n",
    "    text_model=lambda processed_input: torch.from_numpy(text_ort_session.run(['text_embeds', 'last_hidden_state'], processed_input)[0]),\n",
    "    audio_input=audio_sample,\n",
    "    audio_preprocessor=lambda input: np_dict_to_ortvalue({'input_features': pad_mel_dbifier(input)['input_features']}),\n",
    "    audio_model=lambda processed_input: torch.from_numpy(audio_ort_session.run(['audio_embeds', 'last_hidden_state'], processed_input)[0])\n",
    ")\n",
    "print(\"Onnx audio processor took: \", QuickTimer.stop())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check results\n",
    "\n",
    "np.testing.assert_allclose(re_text_embeds.detach().numpy(), ort_text_embeds.detach().numpy(), rtol=1e-03, atol=1e-05)\n",
    "np.testing.assert_allclose(re_audio_embeds.detach().numpy(), ort_audio_embeds.detach().numpy(), rtol=1e-03, atol=1e-05)\n",
    "\n",
    "# All good :)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clap_export",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
