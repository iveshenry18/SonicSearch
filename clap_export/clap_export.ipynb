{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Globals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "tauri_onnx_models_directory = \"../SonicSearch/src-tauri/onnx_models/\"\n",
    "file_timestamp = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "model_name = \"laion/clap-htsat-unfused\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utilities\n",
    "import time\n",
    "\n",
    "def make_filename_or_dirname(filename, extension=None):\n",
    "    extension = \"\" if extension is None else \".\" + extension.strip('.')\n",
    "    filename = filename.strip('.').lstrip('/')\n",
    "    return f'{tauri_onnx_models_directory}{filename}-{file_timestamp}{extension}'\n",
    "\n",
    "# Inspect inputs and outputs\n",
    "def get_shapes_in_nests(node, count=0):\n",
    "    try:\n",
    "        return str(node.shape)\n",
    "    except:\n",
    "        count += 1\n",
    "        try:\n",
    "            return ('\\n' + '\\t'*count).join([f'{key}: {get_shapes_in_nests(value)}' for key, value in node.items()])\n",
    "        except:\n",
    "            if isinstance(node, list):\n",
    "                return ('\\n' + '\\t'*count).join([get_shapes_in_nests(n) for n in node])\n",
    "            else:\n",
    "                return str(node)\n",
    "        \n",
    "class QuickTimer():\n",
    "    \"\"\"hahaha\"\"\"\n",
    "    _start = 0\n",
    "    \n",
    "    def start():\n",
    "        QuickTimer._start = time.time()\n",
    "    \n",
    "    def stop():\n",
    "        return time.time() - QuickTimer._start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Process Dummy Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n",
      "It is strongly recommended to pass the `sampling_rate` argument to this function. Failing to do so can result in silent errors that might be hard to debug.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoProcessor\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"ashraq/esc50\")\n",
    "# NOTE: this dataset has a sampling_rate of 44100, whereas the model expects 48000. It works  dummy input, but don't use it for real data.\n",
    "audio_samples = [data[\"array\"] for data in dataset[\"train\"][\"audio\"][0:32]]\n",
    "\n",
    "input_texts = [\"The sound of a moderate-length input string\", \"The sound of a slightly longer input string\", \"ok\", \n",
    "              \"Now this one is like super super super super suuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuper long!!! and has :) all these characters (f$#%)!\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "It is strongly recommended to pass the `sampling_rate` argument to this function. Failing to do so can result in silent errors that might be hard to debug.\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/iveshenry18/.pyenv/versions/clap_export/lib/python3.11/site-packages/debugpy/_vendored/pydevd/_pydevd_bundle/pydevd_vars.py\", line 624, in change_attr_expression\n",
      "    value = eval(expression, frame.f_globals, frame.f_locals)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"<string>\", line 1, in <module>\n",
      "NameError: name 'array' is not defined\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/iveshenry18/.pyenv/versions/clap_export/lib/python3.11/site-packages/debugpy/_vendored/pydevd/_pydevd_bundle/pydevd_vars.py\", line 624, in change_attr_expression\n",
      "    value = eval(expression, frame.f_globals, frame.f_locals)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"<string>\", line 1, in <module>\n",
      "NameError: name 'tensor' is not defined\n"
     ]
    }
   ],
   "source": [
    "\n",
    "processor = AutoProcessor.from_pretrained(model_name, torchscript=True)\n",
    "processed_inputs = processor(text=input_texts, audios=audio_samples, return_tensors=\"pt\", padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-processed audio:  (220500,)\n",
      "\t(220500,)\n",
      "\t(220500,)\n",
      "\t(220500,)\n",
      "\t(220500,)\n",
      "\t(220500,)\n",
      "\t(220500,)\n",
      "\t(220500,)\n",
      "\t(220500,)\n",
      "\t(220500,)\n",
      "\t(220500,)\n",
      "\t(220500,)\n",
      "\t(220500,)\n",
      "\t(220500,)\n",
      "\t(220500,)\n",
      "\t(220500,)\n",
      "\t(220500,)\n",
      "\t(220500,)\n",
      "\t(220500,)\n",
      "\t(220500,)\n",
      "\t(220500,)\n",
      "\t(220500,)\n",
      "\t(220500,)\n",
      "\t(220500,)\n",
      "\t(220500,)\n",
      "\t(220500,)\n",
      "\t(220500,)\n",
      "\t(220500,)\n",
      "\t(220500,)\n",
      "\t(220500,)\n",
      "\t(220500,)\n",
      "\t(220500,)\n",
      "Pre-processed text:  The sound of a moderate-length input string\n",
      "\tThe sound of a slightly longer input string\n",
      "\tok\n",
      "\tNow this one is like super super super super suuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuper long!!! and has :) all these characters (f$#%)!\n",
      "Processed Model Inputs:  input_ids: torch.Size([4, 42])\n",
      "\tattention_mask: torch.Size([4, 42])\n",
      "\tinput_features: torch.Size([32, 1, 1001, 64])\n",
      "Processed text:  (tensor([[    0,   133,  2369,     9,    10,  7212,    12, 16096,  8135,  6755,\n",
      "             2,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1],\n",
      "        [    0,   133,  2369,     9,    10,  2829,  1181,  8135,  6755,     2,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1],\n",
      "        [    0,  1638,     2,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1],\n",
      "        [    0,  5975,    42,    65,    16,   101,  2422,  2422,  2422,  2422,\n",
      "          2628, 26310, 26310, 26310, 26310, 26310, 26310, 26310, 26310, 26310,\n",
      "         26310, 26310, 26310, 26310, 26310, 26310,  1741,   251, 16506,     8,\n",
      "            34, 44660,    70,   209,  3768,    36,   506,  1629, 10431,  8871,\n",
      "           328,     2]]), tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]))\n",
      "RobertaTokenizerFast(name_or_path='laion/clap-htsat-unfused', vocab_size=50265, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': '<mask>'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
      "\t0: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t1: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t3: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t50264: AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=False, special=True),\n",
      "}\n",
      "Processed Audio:  torch.Size([32, 1, 1001, 64])\n",
      "ClapFeatureExtractor {\n",
      "  \"chunk_length_s\": 10,\n",
      "  \"feature_extractor_type\": \"ClapFeatureExtractor\",\n",
      "  \"feature_size\": 64,\n",
      "  \"fft_window_size\": 1024,\n",
      "  \"frequency_max\": 14000,\n",
      "  \"frequency_min\": 50,\n",
      "  \"hop_length\": 480,\n",
      "  \"max_length_s\": 10,\n",
      "  \"n_fft\": 1024,\n",
      "  \"nb_frequency_bins\": 513,\n",
      "  \"nb_max_frames\": 1000,\n",
      "  \"nb_max_samples\": 480000,\n",
      "  \"padding\": \"repeatpad\",\n",
      "  \"padding_side\": \"right\",\n",
      "  \"padding_value\": 0.0,\n",
      "  \"processor_class\": \"ClapProcessor\",\n",
      "  \"return_attention_mask\": false,\n",
      "  \"sampling_rate\": 48000,\n",
      "  \"top_db\": null,\n",
      "  \"truncation\": \"rand_trunc\"\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Inspect Data\n",
    "import librosa\n",
    "\n",
    "print(\"Pre-processed audio: \", get_shapes_in_nests(audio_samples))\n",
    "librosa.display.waveshow(audio_samples[0], color='b')\n",
    "# 32 (batch) * 220500 (samples)\n",
    "\n",
    "print(\"Pre-processed text: \", get_shapes_in_nests(input_texts))\n",
    "# 4 (batch) * Variable-length string\n",
    "\n",
    "print(\"Processed Model Inputs: \", get_shapes_in_nests(processed_inputs))\n",
    "\n",
    "print(\"Processed text: \", get_shapes_in_nests((processed_inputs[\"input_ids\"], processed_inputs[\"attention_mask\"])))\n",
    "print(processor.tokenizer)\n",
    "# RobertaTokenizerFast\n",
    "# input_ids: 4 (batch) * 42 (tokens), \"1\" is padding.\n",
    "# attention_mask: 4 (batch) * 42 (tokens), \"1\" is for real tokens, \"0\" is for padding.\n",
    "\n",
    "print(\"Processed Audio: \", get_shapes_in_nests(processed_inputs[\"input_features\"]))\n",
    "print(processor.feature_extractor)\n",
    "# ClapFeatureExtractor - Mel Spectrogram + truncation + padding\n",
    "# input_features: 32 (batch) * 1 (channel) * 1001 (?) * 64 (?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save processor configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../SonicSearch/src-tauri/onnx_models/feature_extractor-20231106-235603/preprocessor_config.json']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processor.tokenizer.save_pretrained(make_filename_or_dirname(\"tokenizer\"))\n",
    "processor.feature_extractor.save_pretrained(make_filename_or_dirname(\"feature_extractor\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full CLAP Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load from pretrained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformers Export\n",
    "# https://huggingface.co/docs/transformers/v4.27.2/en/model_doc/clap\n",
    "\n",
    "from transformers import ClapModel\n",
    "model = ClapModel.from_pretrained(model_name)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Full Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model\n",
      "Inputs:  input_ids: torch.Size([4, 42])\n",
      "\tattention_mask: torch.Size([4, 42])\n",
      "\tinput_features: torch.Size([32, 1, 1001, 64])\n",
      "Model finished in  1.8649959564208984 seconds\n",
      "Outputs:  torch.Size([32, 4])\n",
      "\ttorch.Size([4, 32])\n",
      "\ttorch.Size([4, 512])\n",
      "\ttorch.Size([32, 512])\n",
      "\ttorch.Size([4, 42, 768])\n",
      "\ttorch.Size([4, 768])\n",
      "\ttorch.Size([32, 768, 2, 32])\n",
      "\ttorch.Size([32, 768])\n"
     ]
    }
   ],
   "source": [
    "print(\"Running model\")\n",
    "print(\"Inputs: \", get_shapes_in_nests(processed_inputs))\n",
    "QuickTimer.start()\n",
    "outputs = model(**processed_inputs)\n",
    "print(f\"Model finished in  {QuickTimer.stop()} seconds\")\n",
    "print(\"Outputs: \", get_shapes_in_nests(outputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedders-only (broken)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Hack embedding functions as Modules\n",
    "\n",
    "from transformers import ClapProcessor, ClapModel\n",
    "\n",
    "# Hacky way to treat the get_*_features as Modules\n",
    "\n",
    "class ClapModelGetTextFeatures(ClapModel):\n",
    "    def __init__(self, model: ClapModel):\n",
    "        super().__init__(config=model.config)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, **kwargs):\n",
    "        return super().get_text_features(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            output_attentions=False,\n",
    "            output_hidden_states=False,\n",
    "            **kwargs)\n",
    "    \n",
    "class ClapModelGetAudioFeatures(ClapModel):\n",
    "    def __init__(self, model: ClapModel, processor: ClapProcessor):\n",
    "        super().__init__(config=model.config)\n",
    "        self.processor = processor\n",
    "\n",
    "    def forward(self, audio_tensor, **kwargs):\n",
    "        input_features = self.processor.feature_extractor(audio_tensor, return_tensors=\"pt\", padding=True)['input_features']\n",
    "        return super().get_audio_features(input_features=input_features, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coerce Embedder Inputs and Run Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "It is strongly recommended to pass the `sampling_rate` argument to this function. Failing to do so can result in silent errors that might be hard to debug.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running a forward pass on the Text Features Module\n",
      "Using Text Input of shape:  torch.Size([4, 42])\n",
      "\ttorch.Size([4, 42])\n",
      "Text Features Output Shape: torch.Size([4, 512])\n",
      "Text Features took:  0.0975198745727539\n",
      "Running a forward pass on the Audio Features Module\n",
      "Using Audio Input of shape:  (220500,)\n",
      "\t(220500,)\n",
      "\t(220500,)\n",
      "\t(220500,)\n",
      "\t(220500,)\n",
      "\t(220500,)\n",
      "\t(220500,)\n",
      "\t(220500,)\n",
      "\t(220500,)\n",
      "\t(220500,)\n",
      "\t(220500,)\n",
      "\t(220500,)\n",
      "\t(220500,)\n",
      "\t(220500,)\n",
      "\t(220500,)\n",
      "\t(220500,)\n",
      "\t(220500,)\n",
      "\t(220500,)\n",
      "\t(220500,)\n",
      "\t(220500,)\n",
      "\t(220500,)\n",
      "\t(220500,)\n",
      "\t(220500,)\n",
      "\t(220500,)\n",
      "\t(220500,)\n",
      "\t(220500,)\n",
      "\t(220500,)\n",
      "\t(220500,)\n",
      "\t(220500,)\n",
      "\t(220500,)\n",
      "\t(220500,)\n",
      "\t(220500,)\n",
      "Audio Features Output Shape: torch.Size([32, 512])\n",
      "Audio Features took:  2.2357771396636963\n"
     ]
    }
   ],
   "source": [
    "# Reshape inputs to match hacked Modules\n",
    "\n",
    "import numpy\n",
    "from torch import Tensor, from_numpy\n",
    "import time\n",
    "\n",
    "get_text_features_model = ClapModelGetTextFeatures(model)\n",
    "tokenized_inputs = processor.tokenizer(input_texts, return_tensors=\"pt\", padding=True)\n",
    "text_features_dummy_input = (tokenized_inputs[\"input_ids\"], tokenized_inputs[\"attention_mask\"])\n",
    "\n",
    "get_audio_features_model = ClapModelGetAudioFeatures(model, processor)\n",
    "audio_features_dummy_input = audio_samples \n",
    "\n",
    "print(\"Running a forward pass on the Text Features Module\")\n",
    "print(\"Using Text Input of shape: \", get_shapes_in_nests(text_features_dummy_input))\n",
    "start = time.time()\n",
    "text_features_output = get_text_features_model.forward(*text_features_dummy_input)\n",
    "print(\"Text Features Output Shape:\", get_shapes_in_nests(text_features_output))\n",
    "print(\"Text Features took: \", time.time() - start)\n",
    "\n",
    "print(\"Running a forward pass on the Audio Features Module\")\n",
    "print(\"Using Audio Input of shape: \", get_shapes_in_nests(audio_features_dummy_input))\n",
    "start = time.time()\n",
    "audio_features_output = get_audio_features_model.forward(audio_features_dummy_input)\n",
    "print(\"Audio Features Output Shape:\", get_shapes_in_nests(audio_features_output))\n",
    "print(\"Audio Features took: \", time.time() - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export Embedder Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exporting Text Features model to ONNX...\n",
      "Exporting Text Features model to ONNX took:  2.6301209926605225\n",
      "Exporting Audio Features model to ONNX...\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Only tuples, lists and Variables are supported as JIT inputs/outputs. Dictionaries and strings are also accepted, but their usage is not recommended. Here, received an input of unsupported type: numpy.ndarray",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/Users/iveshenry18/Documents/Dev/SonicSearch/clap_export/clap_export.ipynb Cell 16\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/iveshenry18/Documents/Dev/SonicSearch/clap_export/clap_export.ipynb#X10sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mExporting Audio Features model to ONNX...\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/iveshenry18/Documents/Dev/SonicSearch/clap_export/clap_export.ipynb#X10sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m start \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/iveshenry18/Documents/Dev/SonicSearch/clap_export/clap_export.ipynb#X10sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m onnx\u001b[39m.\u001b[39;49mexport(\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/iveshenry18/Documents/Dev/SonicSearch/clap_export/clap_export.ipynb#X10sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m     get_audio_features_model,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/iveshenry18/Documents/Dev/SonicSearch/clap_export/clap_export.ipynb#X10sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m     audio_features_dummy_input,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/iveshenry18/Documents/Dev/SonicSearch/clap_export/clap_export.ipynb#X10sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m     \u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m{\u001b[39;49;00mtauri_onnx_models_directory\u001b[39m}\u001b[39;49;00m\u001b[39mlaion_clap_htsat_unfused_get_audio_features_\u001b[39;49m\u001b[39m{\u001b[39;49;00mfile_timestamp\u001b[39m}\u001b[39;49;00m\u001b[39m.onnx\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/iveshenry18/Documents/Dev/SonicSearch/clap_export/clap_export.ipynb#X10sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m     export_params\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/iveshenry18/Documents/Dev/SonicSearch/clap_export/clap_export.ipynb#X10sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m     do_constant_folding\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/iveshenry18/Documents/Dev/SonicSearch/clap_export/clap_export.ipynb#X10sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m     input_names\u001b[39m=\u001b[39;49m[\u001b[39m\"\u001b[39;49m\u001b[39maudio_tensors\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/iveshenry18/Documents/Dev/SonicSearch/clap_export/clap_export.ipynb#X10sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m     output_names\u001b[39m=\u001b[39;49m[\u001b[39m\"\u001b[39;49m\u001b[39maudio_features\u001b[39;49m\u001b[39m\"\u001b[39;49m]\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/iveshenry18/Documents/Dev/SonicSearch/clap_export/clap_export.ipynb#X10sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/iveshenry18/Documents/Dev/SonicSearch/clap_export/clap_export.ipynb#X10sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mExporting Audio Features model to ONNX took: \u001b[39m\u001b[39m\"\u001b[39m, time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m start)\n",
      "File \u001b[0;32m~/.pyenv/versions/clap_export/lib/python3.11/site-packages/torch/onnx/utils.py:516\u001b[0m, in \u001b[0;36mexport\u001b[0;34m(model, args, f, export_params, verbose, training, input_names, output_names, operator_export_type, opset_version, do_constant_folding, dynamic_axes, keep_initializers_as_inputs, custom_opsets, export_modules_as_functions, autograd_inlining)\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[39m@_beartype\u001b[39m\u001b[39m.\u001b[39mbeartype\n\u001b[1;32m    190\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mexport\u001b[39m(\n\u001b[1;32m    191\u001b[0m     model: Union[torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mModule, torch\u001b[39m.\u001b[39mjit\u001b[39m.\u001b[39mScriptModule, torch\u001b[39m.\u001b[39mjit\u001b[39m.\u001b[39mScriptFunction],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    208\u001b[0m     autograd_inlining: Optional[\u001b[39mbool\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m    209\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    210\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"Exports a model into ONNX format.\u001b[39;00m\n\u001b[1;32m    211\u001b[0m \n\u001b[1;32m    212\u001b[0m \u001b[39m    If ``model`` is not a :class:`torch.jit.ScriptModule` nor a\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    513\u001b[0m \u001b[39m            All errors are subclasses of :class:`errors.OnnxExporterError`.\u001b[39;00m\n\u001b[1;32m    514\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 516\u001b[0m     _export(\n\u001b[1;32m    517\u001b[0m         model,\n\u001b[1;32m    518\u001b[0m         args,\n\u001b[1;32m    519\u001b[0m         f,\n\u001b[1;32m    520\u001b[0m         export_params,\n\u001b[1;32m    521\u001b[0m         verbose,\n\u001b[1;32m    522\u001b[0m         training,\n\u001b[1;32m    523\u001b[0m         input_names,\n\u001b[1;32m    524\u001b[0m         output_names,\n\u001b[1;32m    525\u001b[0m         operator_export_type\u001b[39m=\u001b[39;49moperator_export_type,\n\u001b[1;32m    526\u001b[0m         opset_version\u001b[39m=\u001b[39;49mopset_version,\n\u001b[1;32m    527\u001b[0m         do_constant_folding\u001b[39m=\u001b[39;49mdo_constant_folding,\n\u001b[1;32m    528\u001b[0m         dynamic_axes\u001b[39m=\u001b[39;49mdynamic_axes,\n\u001b[1;32m    529\u001b[0m         keep_initializers_as_inputs\u001b[39m=\u001b[39;49mkeep_initializers_as_inputs,\n\u001b[1;32m    530\u001b[0m         custom_opsets\u001b[39m=\u001b[39;49mcustom_opsets,\n\u001b[1;32m    531\u001b[0m         export_modules_as_functions\u001b[39m=\u001b[39;49mexport_modules_as_functions,\n\u001b[1;32m    532\u001b[0m         autograd_inlining\u001b[39m=\u001b[39;49mautograd_inlining,\n\u001b[1;32m    533\u001b[0m     )\n",
      "File \u001b[0;32m~/.pyenv/versions/clap_export/lib/python3.11/site-packages/torch/onnx/utils.py:1596\u001b[0m, in \u001b[0;36m_export\u001b[0;34m(model, args, f, export_params, verbose, training, input_names, output_names, operator_export_type, export_type, opset_version, do_constant_folding, dynamic_axes, keep_initializers_as_inputs, fixed_batch_size, custom_opsets, add_node_names, onnx_shape_inference, export_modules_as_functions, autograd_inlining)\u001b[0m\n\u001b[1;32m   1593\u001b[0m     dynamic_axes \u001b[39m=\u001b[39m {}\n\u001b[1;32m   1594\u001b[0m _validate_dynamic_axes(dynamic_axes, model, input_names, output_names)\n\u001b[0;32m-> 1596\u001b[0m graph, params_dict, torch_out \u001b[39m=\u001b[39m _model_to_graph(\n\u001b[1;32m   1597\u001b[0m     model,\n\u001b[1;32m   1598\u001b[0m     args,\n\u001b[1;32m   1599\u001b[0m     verbose,\n\u001b[1;32m   1600\u001b[0m     input_names,\n\u001b[1;32m   1601\u001b[0m     output_names,\n\u001b[1;32m   1602\u001b[0m     operator_export_type,\n\u001b[1;32m   1603\u001b[0m     val_do_constant_folding,\n\u001b[1;32m   1604\u001b[0m     fixed_batch_size\u001b[39m=\u001b[39;49mfixed_batch_size,\n\u001b[1;32m   1605\u001b[0m     training\u001b[39m=\u001b[39;49mtraining,\n\u001b[1;32m   1606\u001b[0m     dynamic_axes\u001b[39m=\u001b[39;49mdynamic_axes,\n\u001b[1;32m   1607\u001b[0m )\n\u001b[1;32m   1609\u001b[0m \u001b[39m# TODO: Don't allocate a in-memory string for the protobuf\u001b[39;00m\n\u001b[1;32m   1610\u001b[0m defer_weight_export \u001b[39m=\u001b[39m (\n\u001b[1;32m   1611\u001b[0m     export_type \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m _exporter_states\u001b[39m.\u001b[39mExportTypes\u001b[39m.\u001b[39mPROTOBUF_FILE\n\u001b[1;32m   1612\u001b[0m )\n",
      "File \u001b[0;32m~/.pyenv/versions/clap_export/lib/python3.11/site-packages/torch/onnx/utils.py:1135\u001b[0m, in \u001b[0;36m_model_to_graph\u001b[0;34m(model, args, verbose, input_names, output_names, operator_export_type, do_constant_folding, _disable_torch_constant_prop, fixed_batch_size, training, dynamic_axes)\u001b[0m\n\u001b[1;32m   1132\u001b[0m     args \u001b[39m=\u001b[39m (args,)\n\u001b[1;32m   1134\u001b[0m model \u001b[39m=\u001b[39m _pre_trace_quant_model(model, args)\n\u001b[0;32m-> 1135\u001b[0m graph, params, torch_out, module \u001b[39m=\u001b[39m _create_jit_graph(model, args)\n\u001b[1;32m   1136\u001b[0m params_dict \u001b[39m=\u001b[39m _get_named_param_dict(graph, params)\n\u001b[1;32m   1138\u001b[0m \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/.pyenv/versions/clap_export/lib/python3.11/site-packages/torch/onnx/utils.py:1011\u001b[0m, in \u001b[0;36m_create_jit_graph\u001b[0;34m(model, args)\u001b[0m\n\u001b[1;32m   1006\u001b[0m     graph \u001b[39m=\u001b[39m _C\u001b[39m.\u001b[39m_propagate_and_assign_input_shapes(\n\u001b[1;32m   1007\u001b[0m         graph, flattened_args, param_count_list, \u001b[39mFalse\u001b[39;00m, \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m   1008\u001b[0m     )\n\u001b[1;32m   1009\u001b[0m     \u001b[39mreturn\u001b[39;00m graph, params, torch_out, \u001b[39mNone\u001b[39;00m\n\u001b[0;32m-> 1011\u001b[0m graph, torch_out \u001b[39m=\u001b[39m _trace_and_get_graph_from_model(model, args)\n\u001b[1;32m   1012\u001b[0m _C\u001b[39m.\u001b[39m_jit_pass_onnx_lint(graph)\n\u001b[1;32m   1013\u001b[0m state_dict \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mjit\u001b[39m.\u001b[39m_unique_state_dict(model)\n",
      "File \u001b[0;32m~/.pyenv/versions/clap_export/lib/python3.11/site-packages/torch/onnx/utils.py:915\u001b[0m, in \u001b[0;36m_trace_and_get_graph_from_model\u001b[0;34m(model, args)\u001b[0m\n\u001b[1;32m    913\u001b[0m prev_autocast_cache_enabled \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mis_autocast_cache_enabled()\n\u001b[1;32m    914\u001b[0m torch\u001b[39m.\u001b[39mset_autocast_cache_enabled(\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m--> 915\u001b[0m trace_graph, torch_out, inputs_states \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mjit\u001b[39m.\u001b[39;49m_get_trace_graph(\n\u001b[1;32m    916\u001b[0m     model,\n\u001b[1;32m    917\u001b[0m     args,\n\u001b[1;32m    918\u001b[0m     strict\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    919\u001b[0m     _force_outplace\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    920\u001b[0m     _return_inputs_states\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    921\u001b[0m )\n\u001b[1;32m    922\u001b[0m torch\u001b[39m.\u001b[39mset_autocast_cache_enabled(prev_autocast_cache_enabled)\n\u001b[1;32m    924\u001b[0m warn_on_static_input_change(inputs_states)\n",
      "File \u001b[0;32m~/.pyenv/versions/clap_export/lib/python3.11/site-packages/torch/jit/_trace.py:1285\u001b[0m, in \u001b[0;36m_get_trace_graph\u001b[0;34m(f, args, kwargs, strict, _force_outplace, return_inputs, _return_inputs_states)\u001b[0m\n\u001b[1;32m   1283\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(args, \u001b[39mtuple\u001b[39m):\n\u001b[1;32m   1284\u001b[0m     args \u001b[39m=\u001b[39m (args,)\n\u001b[0;32m-> 1285\u001b[0m outs \u001b[39m=\u001b[39m ONNXTracedModule(\n\u001b[1;32m   1286\u001b[0m     f, strict, _force_outplace, return_inputs, _return_inputs_states\n\u001b[1;32m   1287\u001b[0m )(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1288\u001b[0m \u001b[39mreturn\u001b[39;00m outs\n",
      "File \u001b[0;32m~/.pyenv/versions/clap_export/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.pyenv/versions/clap_export/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/clap_export/lib/python3.11/site-packages/torch/jit/_trace.py:100\u001b[0m, in \u001b[0;36mONNXTracedModule.forward\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs: torch\u001b[39m.\u001b[39mTensor):\n\u001b[0;32m--> 100\u001b[0m     in_vars, in_desc \u001b[39m=\u001b[39m _flatten(args)\n\u001b[1;32m    101\u001b[0m     \u001b[39m# NOTE: use full state, because we need it for BatchNorm export\u001b[39;00m\n\u001b[1;32m    102\u001b[0m     \u001b[39m# This differs from the compiler path, which doesn't support it at the moment.\u001b[39;00m\n\u001b[1;32m    103\u001b[0m     module_state \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(_unique_state_dict(\u001b[39mself\u001b[39m, keep_vars\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\u001b[39m.\u001b[39mvalues())\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Only tuples, lists and Variables are supported as JIT inputs/outputs. Dictionaries and strings are also accepted, but their usage is not recommended. Here, received an input of unsupported type: numpy.ndarray"
     ]
    }
   ],
   "source": [
    "# Onnx Export - Feature Embedding Only\n",
    "\n",
    "from torch import onnx\n",
    "import time\n",
    "\n",
    "# Text Features\n",
    "print(\"Exporting Text Features model to ONNX...\")\n",
    "start = time.time()\n",
    "onnx.export(\n",
    "    get_text_features_model,\n",
    "    text_features_dummy_input,\n",
    "    make_filename_or_dirname(\"laion_clap_htsat_unfused_get_text_features\", \"onnx\"),\n",
    "    export_params=True,\n",
    "    input_names=[\"input_ids\", \"attention_masks\"],\n",
    "    output_names=[\"text_features\"]\n",
    ")\n",
    "print(\"Exporting Text Features model to ONNX took: \", time.time() - start)\n",
    "\n",
    "# Audio Features\n",
    "print(\"Exporting Audio Features model to ONNX...\")\n",
    "start = time.time()\n",
    "onnx.export(\n",
    "    get_audio_features_model,\n",
    "    audio_features_dummy_input,\n",
    "    make_filename_or_dirname(\"laion_clap_htsat_unfused_get_audio_features\", \"onnx\"),\n",
    "    export_params=True,\n",
    "    do_constant_folding=False,\n",
    "    input_names=[\"audio_tensors\"],\n",
    "    output_names=[\"audio_features\"]\n",
    ")\n",
    "print(\"Exporting Audio Features model to ONNX took: \", time.time() - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export Full Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exporting model to ONNX...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/iveshenry18/.pyenv/versions/clap_export/lib/python3.11/site-packages/transformers/models/clap/modeling_clap.py:870: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if time_length > spec_width or freq_length > spec_heigth:\n",
      "/Users/iveshenry18/.pyenv/versions/clap_export/lib/python3.11/site-packages/transformers/models/clap/modeling_clap.py:874: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if time_length < spec_width:\n",
      "/Users/iveshenry18/.pyenv/versions/clap_export/lib/python3.11/site-packages/transformers/models/clap/modeling_clap.py:878: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if freq_length < spec_heigth:\n",
      "/Users/iveshenry18/.pyenv/versions/clap_export/lib/python3.11/site-packages/transformers/models/clap/modeling_clap.py:385: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if height != self.img_size[0] or width != self.img_size[1]:\n",
      "/Users/iveshenry18/.pyenv/versions/clap_export/lib/python3.11/site-packages/transformers/models/clap/modeling_clap.py:105: TracerWarning: Converting a tensor to a Python integer might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  batch_size = int(windows.shape[0] / (height * width / window_size / window_size))\n",
      "/Users/iveshenry18/.pyenv/versions/clap_export/lib/python3.11/site-packages/torch/onnx/_internal/jit_utils.py:307: UserWarning: Constant folding - Only steps=1 can be constant folded for opset >= 10 onnx::Slice op. Constant folding not applied. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/jit/passes/onnx/constant_fold.cpp:181.)\n",
      "  _C._jit_pass_onnx_node_shape_type_inference(node, params_dict, opset_version)\n",
      "/Users/iveshenry18/.pyenv/versions/clap_export/lib/python3.11/site-packages/torch/onnx/utils.py:702: UserWarning: Constant folding - Only steps=1 can be constant folded for opset >= 10 onnx::Slice op. Constant folding not applied. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/jit/passes/onnx/constant_fold.cpp:181.)\n",
      "  _C._jit_pass_onnx_graph_shape_type_inference(\n",
      "/Users/iveshenry18/.pyenv/versions/clap_export/lib/python3.11/site-packages/torch/onnx/utils.py:1209: UserWarning: Constant folding - Only steps=1 can be constant folded for opset >= 10 onnx::Slice op. Constant folding not applied. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/jit/passes/onnx/constant_fold.cpp:181.)\n",
      "  _C._jit_pass_onnx_graph_shape_type_inference(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exporting model to ONNX took:  31.832356214523315\n"
     ]
    }
   ],
   "source": [
    "# Onnx Export - Full Model\n",
    "\n",
    "from torch import onnx\n",
    "import time\n",
    "\n",
    "onnx_inputs = (processed_inputs[\"input_ids\"], processed_inputs[\"input_features\"], False, processed_inputs[\"attention_mask\"])\n",
    "onnx_input_names = [\"input_ids\", \"input_features\", \"is_longer\", \"attention_mask\"]\n",
    "\n",
    "onnx_output_names = []\n",
    "\n",
    "print(\"Exporting model to ONNX...\")\n",
    "start = time.time()\n",
    "onnx.export(\n",
    "    model,\n",
    "    onnx_inputs,\n",
    "    make_filename_or_dirname(\"laion_clap_htsat_unfused\", \"onnx\"),\n",
    "    export_params=True,\n",
    "    input_names=onnx_input_names,\n",
    "    output_names=model(**processed_inputs, return_dict=True).keys(),\n",
    "    \n",
    ")\n",
    "print(\"Exporting model to ONNX took: \", time.time() - start)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clap_export",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
